apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: rook-ceph
  namespace: argocd
spec:
  destination:
    namespace: rook-ceph2
    server: {{ .Values.spec.destination.server }}
  project: default
  sources:
  - chart: rook-ceph
    repoURL: https://charts.rook.io/release
    targetRevision: v1.19.1
    helm:
      release-name: rook-ceph
      valuesObject:
        serviceMonitor:
          enabled: true
          interval: 1m
        monitoring:
          enabled: true
  - chart: rook-ceph-cluster
    repoURL: https://charts.rook.io/release
    targetRevision: v1.19.1
    helm:
      release-name: rook-ceph-cluster
      valuesObject:
        clusterName: rook-ceph
        operatorNamespace: rook-ceph2
        monitoring:
          enabled: true
          createPrometheusRules: true
        cephClusterSpec:
          # cephVersion:
          #   image: quay.io/ceph/ceph:v18.2.2
          cleanupPolicy:
            sanitizeDisks:
              dataSource: zero
              iteration: 1
              method: quick
          crashCollector: {}
          csi:
            cephfs: {}
            readAffinity:
              enabled: false
          dashboard:
            enabled: true
            ssl: true
          dataDirHostPath: /var/lib/rook
          disruptionManagement:
            managePodBudgets: true
            osdMaintenanceTimeout: 30
          external: {}
          healthCheck:
            daemonHealth:
              mon:
                interval: 45s
              osd:
                interval: 1m0s
              status:
                interval: 1m0s
            livenessProbe:
              mgr: {}
              mon: {}
              osd: {}
          logCollector:
            enabled: true
            maxLogSize: 500M
            periodicity: daily
          mgr:
            count: 2
          mon:
            count: 3
          network:
            connections:
              compression: {}
              encryption: {}
            multiClusterService: {}
          priorityClassNames:
            mgr: system-cluster-critical
            mon: system-node-critical
            osd: system-node-critical
          resources:
            cleanup:
              limits:
                memory: 1Gi
              requests:
                cpu: 100m
                memory: 100Mi
            crashcollector:
              limits:
                memory: 60Mi
              requests:
                cpu: 100m
                memory: 60Mi
            exporter:
              limits:
                memory: 128Mi
              requests:
                cpu: 50m
                memory: 50Mi
            logcollector:
              limits:
                memory: 1Gi
              requests:
                cpu: 100m
                memory: 100Mi
            mgr:
              limits:
                memory: 1Gi
              requests:
                cpu: 200m
                memory: 512Mi
            mgr-sidecar:
              limits:
                memory: 100Mi
              requests:
                cpu: 100m
                memory: 40Mi
            mon:
              limits:
                memory: 2Gi
              requests:
                cpu: 200m
                memory: 1Gi
            osd:
              limits:
                memory: 4Gi
              requests:
                cpu: 200m
                memory: 4Gi
            prepareosd:
              requests:
                cpu: 500m
                memory: 50Mi
          security:
            keyRotation:
              enabled: false
            kms: {}
          storage:
            flappingRestartIntervalHours: 0
            nodes:
            - devices:
              - name: /dev/disk/by-id/usb-JMicron_Tech_DD564198838C4-0:0
              name: talos-95t-m8m
            - devices:
              - name: /dev/disk/by-id/usb-Realtek_RTL9210B-CG_012345678932-0:0
              - name: /dev/disk/by-id/t10.ATASPCC_M.2_SSDSP20241029A0100703
              - name: /dev/disk/by-id/ata-SPCC_M.2_SSD_SP20241029A0100703
              name: talos-jly-e8b
            - devices:
              - name: /dev/disk/by-id/nvme-Samsung_SSD_970_EVO_Plus_1TB_S6P7NF0T303215D
              name: talos-k1y-dkc
            store: {}
            useAllDevices: false
            useAllNodes: false
          upgradeOSDRequiresHealthyPGs: false
          waitTimeoutForHealthyOSDInMinutes: 10
        cephBlockPools:
          - name: ceph-blockpool
            storageClass:
              enabled: true
              name: rook-ceph-block
              isDefault: true
              reclaimPolicy: Retain
              parameters:
                csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
                csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph2
                csi.storage.k8s.io/fstype: ext4
                csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
                csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph2
                csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
                csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph2
                imageFeatures: layering,fast-diff,object-map,deep-flatten,exclusive-lock
            spec:
              application: ""
              erasureCoded:
                codingChunks: 0
                dataChunks: 0
              failureDomain: host
              mirroring: {}
              quotas: {}
              replicated:
                size: 3
              statusCheck:
                mirror: {}
        cephBlockPoolsVolumeSnapshotClass:
          enabled: true
          name: ceph-block
          isDefault: false
          deletionPolicy: Delete
          annotations: {}
          labels:
            velero.io/csi-volumesnapshot-class: "true"
          # see https://rook.io/docs/rook/latest/ceph-csi-snapshot.html#rbd-snapshots for available configuration
          parameters:
            clusterID: rook-ceph2 # namespace:cluster
            csi.storage.k8s.io/snapshotter-secret-name: rook-csi-rbd-provisioner
            csi.storage.k8s.io/snapshotter-secret-namespace: rook-ceph2 # namespace:cluster
        cephFileSystems:
          - name: ceph-filesystem
            storageClass:
              enabled: true
              name: rook-ceph-filesystem
              isDefault: false
              reclaimPolicy: Retain
              parameters:
                clusterID: rook-ceph2
                csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
                csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph2
                csi.storage.k8s.io/fstype: ext4
                csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
                csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph2
                csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
                csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph2
                fsName: ceph-filesystem
                pool: ceph-filesystem-data0
            spec:
              dataPools:
              - failureDomain: host
                name: data0
                replicated:
                  size: 3
              metadataPool:
                application: ""
                erasureCoded:
                  codingChunks: 0
                  dataChunks: 0
                mirroring: {}
                quotas: {}
                replicated:
                  size: 3
                statusCheck:
                  mirror: {}
              metadataServer:
                activeCount: 1
                activeStandby: true
                placement: {}
                priorityClassName: system-cluster-critical
                resources:
                  limits:
                    memory: 4Gi
                  requests:
                    cpu: 100m
                    memory: 1Gi
              statusCheck:
                mirror: {}
        cephObjectStores:
          - name: ceph-objectstore
            storageClass:
              enabled: true
              name: rook-ceph-bucket
              isDefault: false
              reclaimPolicy: Retain
              parameters:
                objectStoreName: ceph-objectstore
                objectStoreNamespace: rook-ceph2
                region: us-east-1
            spec:
              auth: {}
              dataPool:
                application: ""
                erasureCoded:
                  codingChunks: 1
                  dataChunks: 2
                failureDomain: host
                mirroring: {}
                quotas: {}
                replicated:
                  size: 0
                statusCheck:
                  mirror: {}
              gateway:
                instances: 1
                placement: {}
                port: 80
                priorityClassName: system-cluster-critical
                resources:
                  limits:
                    memory: 2Gi
                  requests:
                    cpu: 100m
                    memory: 512Mi
              healthCheck: {}
              metadataPool:
                application: ""
                erasureCoded:
                  codingChunks: 0
                  dataChunks: 0
                failureDomain: host
                mirroring: {}
                quotas: {}
                replicated:
                  size: 3
                statusCheck:
                  mirror: {}
              preservePoolsOnDelete: true
              protocols:
                swift: null
              sharedPools:
                preserveRadosNamespaceDataOnDelete: false
              zone:
                name: ""

